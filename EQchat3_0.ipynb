{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PunDin0827/EQ_RAG/blob/main/EQchat3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679bc4c2",
      "metadata": {
        "id": "679bc4c2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import chromadb\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import regex as re_u\n",
        "from datetime import datetime\n",
        "from rank_bm25 import BM25Okapi\n",
        "from collections import defaultdict\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModel , AutoTokenizer\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.memory import ChatSummaryMemoryBuffer\n",
        "from llama_index.core.llms import ChatMessage , MessageRole\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.vector_stores import FilterCondition, FilterOperator\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext , Document\n",
        "from llama_index.core.vector_stores import MetadataFilter, MetadataFilters, FilterCondition, FilterOperator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "519ba93b",
      "metadata": {
        "id": "519ba93b"
      },
      "source": [
        "## LLM model ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce45eb07",
      "metadata": {
        "id": "ce45eb07"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "llm = LlamaCPP(\n",
        "        model_path = \"Qwen3-14B-Q4_K_M.gguf\",\n",
        "        model_kwargs = {\n",
        "            \"n_ctx\" : 4096 ,\n",
        "            \"n_gpu_layers\" : -1,\n",
        "            \"n_batch\" : 192,\n",
        "            \"top_k\" : 0,\n",
        "            \"top_p\" : 0.9,\n",
        "            \"n_threads\" : 32,\n",
        "            \"temperature\" : 0.6,\n",
        "            \"max_tokens\" : 512\n",
        "        }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4863ba8",
      "metadata": {
        "id": "f4863ba8"
      },
      "source": [
        "## vector DB ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f47aeb",
      "metadata": {
        "id": "89f47aeb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# vectorDB\n",
        "# 1. 準備 chroma\n",
        "chroma_client = chromadb.PersistentClient(\"chromadb\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"RAG_EQ\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d25241a1",
      "metadata": {
        "id": "d25241a1"
      },
      "source": [
        "## Data embedding ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6f614c",
      "metadata": {
        "id": "ee6f614c",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Alarm code\n",
        "PAT_EQ = re.compile(###)\n",
        "PAT_EQ_type = re.compile(###)\n",
        "PAT_MR_code = re.compile(###)\n",
        "PAT_Err = re.compile(###)\n",
        "PAT_AI_COMBO = re.compile(###)\n",
        "PAT_AI_SINGLE = re.compile(###)\n",
        "PAT_FANUC_code = re.compile(###)\n",
        "PAT_alarm = re.compile(###)\n",
        "def norm(s: str) -> str:\n",
        "    return(s.replace('x','-').replace('X','-')\n",
        "              .replace('．','.')\n",
        "              .replace('–','-').replace('—','-')\n",
        "              .upper().strip())\n",
        "\n",
        "def Alarm_tittle(meta:dict):\n",
        "    codes = meta.get(\"AlarmCode\" , []) or []\n",
        "    meta[\"has_alarm\"] = bool(codes)\n",
        "    if codes:\n",
        "        fam = {c.split(\"#\", 1)[0] for c in codes if \"#\" in c}\n",
        "        meta[\"alarm_family\"] = sorted(fam)\n",
        "    else:\n",
        "        meta[\"alarm_family\"] = []\n",
        "    return meta\n",
        "\n",
        "\n",
        "def Alarm_metadata(filepath):\n",
        "    with open(filepath, encoding=\"utf8\") as f:\n",
        "        raw = f.read()\n",
        "    chunks = [c.strip() for c in raw.split(\"---\") if c.strip()]\n",
        "    nodes2 = []\n",
        "\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        # ---------- 機台/機型 ----------\n",
        "        m_eq = PAT_EQ.search(chunk)\n",
        "        eq_name = m_eq.group(1).upper() if m_eq else \"\"\n",
        "        etypes = [mt.group(1) for mt in PAT_EQ_type.finditer(chunk)]\n",
        "        eq_type = etypes[0] if etypes else \"\"\n",
        "\n",
        "        # ---------- Alarm Codes ----------\n",
        "        codes = set()\n",
        "        for m in PAT_MR_code.finditer(chunk):\n",
        "            codes.add(f\"MR#{m.group(1).upper()}\")\n",
        "        for m in PAT_Err.finditer(chunk):\n",
        "            codes.add(f\"ERR#{norm(m.group(1))}\")\n",
        "        for m in PAT_AI_COMBO.finditer(chunk):\n",
        "            for n in re.split(r'[,\\s、/＋\\+]+', m.group(1)):\n",
        "                if n.isdigit():\n",
        "                    codes.add(f\"AI#{n}\")\n",
        "        for m in PAT_AI_SINGLE.finditer(chunk):\n",
        "            codes.add(f\"AI#{m.group(1)}\")\n",
        "        for m in PAT_alarm.finditer(chunk):\n",
        "            codes.add(f\"Alarm#{m.group(1)}\")\n",
        "        for m in PAT_FANUC_code.finditer(chunk):\n",
        "            alpha = m.group(1).upper()\n",
        "            num = m.group(2)\n",
        "            codes.add(f\"FANUC#{alpha}-{num}\")\n",
        "\n",
        "        # ---------- 產生節點：一碼一 node ----------\n",
        "        if codes:\n",
        "            for code in sorted(codes):\n",
        "                meta = {\n",
        "                    \"eq_name\" : eq_name,            # str\n",
        "                    \"eq_type\" : eq_type,            # str\n",
        "                    \"has_alarm\" : 1,\n",
        "                    \"alarm_code\" : code,\n",
        "                    \"code_family\" : code.split(\"#\", 1)[0],  # AI/MR/ERR/FANUC（也單值）\n",
        "                }\n",
        "                nodes2.append(Document(text=chunk, metadata=meta))\n",
        "        else:\n",
        "            # 沒碼也可建一筆\n",
        "            meta = {\n",
        "                \"eq_name\" : eq_name,\n",
        "                \"eq_type\" : eq_type,\n",
        "                \"has_alarm\" : 0,                    # int\n",
        "                \"alarm_code\" : \"\",\n",
        "                \"code_family\" : \"\",\n",
        "            }\n",
        "            nodes2.append(Document(text=chunk, metadata=meta))\n",
        "\n",
        "    return nodes2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c3011ab",
      "metadata": {
        "id": "1c3011ab"
      },
      "outputs": [],
      "source": [
        "Alarm_metadata(\"AlarmCode.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62552a3f",
      "metadata": {
        "id": "62552a3f"
      },
      "outputs": [],
      "source": [
        "nodes = Alarm_metadata(\"alldata.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11362d99",
      "metadata": {
        "id": "11362d99"
      },
      "source": [
        "## Embedding ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ebd14f",
      "metadata": {
        "id": "b7ebd14f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # 3. embedding model\n",
        "# embed_model = HuggingFaceEmbedding(model_name = \"QWEN3_4B_embedding\",\n",
        "#                            device = \"cuda\" ,\n",
        "#                            embed_batch_size=32,\n",
        "#                            model_kwargs={\"torch_dtype\": \"float16\"},)\n",
        "# # 4. 建索引\n",
        "# index = VectorStoreIndex(nodes, embed_model = embed_model, storage_context = storage_context)\n",
        "\n",
        "# index.storage_context.persist()\n",
        "\n",
        "# # 5. 查詢資料筆數\n",
        "# ids = chroma_collection.get()[\"ids\"]\n",
        "# print(f\"RAG_EQ collection 筆數: {len(ids)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a89537a",
      "metadata": {
        "id": "1a89537a"
      },
      "source": [
        "## Inference ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dddd0b2",
      "metadata": {
        "id": "4dddd0b2"
      },
      "outputs": [],
      "source": [
        "EQ_dict = {\n",
        "   ###\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def get_EQ(input):\n",
        "    key = input.upper().replace(\"-\",\"\").replace(\"_\",\"\").replace(\" \",\"\")\n",
        "    for main , list in EQ_dict.items():\n",
        "        full_list = [main] + list\n",
        "        if key in [list.upper().replace(\"_\",\"\").replace(\"_\",\"\").replace(\" \",\"\") for list in full_list]:\n",
        "            return main # 主名字\n",
        "    return input # 沒找到就回傳原輸入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ff22ce",
      "metadata": {
        "id": "e3ff22ce"
      },
      "outputs": [],
      "source": [
        "print(get_EQ(\"###\"))\n",
        "print(get_EQ(\"###\"))\n",
        "print(get_EQ(\"###\"))\n",
        "print(get_EQ(\"###\"))\n",
        "print(get_EQ(\"###\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ebb3ebf",
      "metadata": {
        "id": "8ebb3ebf"
      },
      "outputs": [],
      "source": [
        "PAT_EQ = re.compile(###)\n",
        "PAT_EQ_type = re.compile(###)\n",
        "PAT_MR = re.compile(###)\n",
        "PAT_ERR_FAMILY = ###\n",
        "PAT_ERR_CODE = re.compile(###)\n",
        "PAT_AI = re.compile(###)\n",
        "PAT_alarm = re.compile(###)\n",
        "PAT_FANUC = re.compile(###)\n",
        "\n",
        "def norm(s:str)->str:\n",
        "    return (s.replace('x','-').replace('X','-')\n",
        "             .replace('．','.')\n",
        "             .replace('–','-').replace('—','-')\n",
        "             .upper().strip())\n",
        "\n",
        "def in_filter(key: str , values:list[str]) ->MetadataFilter|None:\n",
        "    vals = [v for v in values if v]\n",
        "    if not vals:\n",
        "        return None\n",
        "    return MetadataFilter(key = key , value = vals , operator = FilterOperator.IN)\n",
        "\n",
        "def parse_filters(question: str) -> MetadataFilters | None:\n",
        "    flat_filters: list[MetadataFilter] = []     # 只加入「有命中」\n",
        "\n",
        "    # === eq_name 可能多個 -> OR 群組（EQ） ===\n",
        "    eq_names = []\n",
        "    for m in PAT_EQ.finditer(question):\n",
        "        eq = get_EQ(m.group(1))\n",
        "        if eq:\n",
        "            eq_names.append(eq.upper())\n",
        "    f_eq = in_filter(\"eq_name\", eq_names)\n",
        "    if f_eq:\n",
        "        flat_filters.append(f_eq)\n",
        "\n",
        "    # === eq_type 可能多個 -> OR 群組\n",
        "    etypes = [mt.group(1) for mt in PAT_EQ_type.finditer(question)]\n",
        "    f_type = in_filter(\"eq_type\", etypes)\n",
        "    if f_type:\n",
        "        flat_filters.append(f_type)\n",
        "\n",
        "    # === alarm_code 把各種碼正規化成「型別#數值」 -> OR 群組（EQ） ===\n",
        "    codes = []\n",
        "\n",
        "    for mm in PAT_MR.finditer(question):\n",
        "        codes.append(f\"MR#{mm.group(1).upper()}\")\n",
        "\n",
        "    for mc in PAT_ERR_CODE.finditer(question):\n",
        "         codes.append(f\"ERR#{norm(mc.group(1))}\")\n",
        "\n",
        "    for ma in PAT_AI.finditer(question):\n",
        "        codes.append(f\"AI#{ma.group(1)}\")\n",
        "\n",
        "    for mf in PAT_FANUC.finditer(question):\n",
        "        alpha = mf.group(1).upper()\n",
        "        num = mf.group(2)\n",
        "        codes.append(f\"FANUC#{alpha}-{num}\")\n",
        "\n",
        "    for m in PAT_alarm.finditer(question):\n",
        "        code = m.group(1).upper()\n",
        "        codes.append(f\"Alarm#{code}\")\n",
        "\n",
        "    f_code = in_filter(\"alarm_code\", codes)\n",
        "    if f_code:\n",
        "        flat_filters.append(f_code)\n",
        "\n",
        "    # === 組裝：只把「有命中」的群組 AND 起來 若全部都沒有，回 None ===\n",
        "    if not flat_filters:\n",
        "        return None\n",
        "    return MetadataFilters(filters=flat_filters, condition=FilterCondition.AND)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6120f675",
      "metadata": {
        "id": "6120f675"
      },
      "source": [
        "## BM25 ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eb4c519",
      "metadata": {
        "id": "5eb4c519"
      },
      "outputs": [],
      "source": [
        "RE_TOKEN = re.compile(r\"[A-Za-z0-9]+(?:[-_.:/#][A-Za-z0-9]+)*\" , re.I)\n",
        "RE_CJK = re.compile(r\"[\\u4E00-\\u9FFF\\u3400-\\u4DBF]+\")\n",
        "def nfkc(s:str)->str:\n",
        "    return unicodedata.normalize(\"NFKC\" , s)\n",
        "\n",
        "def norm_units(s:str)->str:\n",
        "    s = re.sub(r\"(?<=\\d)([A-Za-z%]+)\", r\" \\1\", s)\n",
        "    return s\n",
        "\n",
        "def cleanup(s:str)->str:\n",
        "    s = re_u.sub(r\"[\\p{C}--[\\n\\t]]+\" , \"\" , s )\n",
        "    return s\n",
        "\n",
        "def punctunify(s:str)->str:\n",
        "    table = str.maketrans(\"，。；：【】（）％／－～\", \",.;:[]()%/-~\")\n",
        "    return s.translate(table)\n",
        "\n",
        "def code_family_variants(tok: str):\n",
        "    m = re.match(r\"(err|mr|ai|alarm)[-_:# ]?([0-9]+(?:[.x-][0-9]+)*)$\", tok, re.I)\n",
        "    if not m: return []\n",
        "    fam = m.group(1).lower()\n",
        "    num = m.group(2).lower()\n",
        "    core = re.split(r\"[.x-]\", num)[0]\n",
        "    return [f\"{fam}#{num}\", f\"{fam}#{core}\", fam, num, core]\n",
        "\n",
        "\n",
        "def tokenize(text:str , add_trigram: bool = False):\n",
        "    s = nfkc(text)\n",
        "    s = cleanup(s)\n",
        "    s = punctunify(s)\n",
        "    s = norm_units(s)\n",
        "    s = re.sub(r\"\\s+\" ,\" \" , s).strip()\n",
        "\n",
        "    tokens , spans = [] , []\n",
        "\n",
        "    for m in RE_TOKEN.finditer(s):\n",
        "        t = m.group(0)\n",
        "        tokens.append(t)\n",
        "        tokens.extend(code_family_variants(t))\n",
        "        spans.append(m.span())\n",
        "\n",
        "    cursor = 0\n",
        "    for(a,b) in spans + [(len(s) , len(s))]:\n",
        "        if cursor < a:\n",
        "            chunk = s[cursor:a]\n",
        "            for cjkm in RE_CJK.finditer(chunk):\n",
        "                seg = cjkm.group(0)\n",
        "                seg = re.sub(r\"\\s+\", \"\", seg)\n",
        "                tokens.extend([seg[i:i+2] for i in range(len(seg)-1)])\n",
        "                if add_trigram and len(seg) >=3:\n",
        "                    tokens.extend([seg[i:i+3] for i in range(len(seg)-2)])\n",
        "        cursor = b\n",
        "    return tokens\n",
        "\n",
        "\n",
        "corpus_tokens = []\n",
        "doc_ids = []\n",
        "corpus   = []\n",
        "text_map = {}\n",
        "\n",
        "for n in nodes:\n",
        "    nid = getattr(n, \"node_id\", getattr(n, \"id_\", None))\n",
        "    text = getattr(n, \"text\", \"\")\n",
        "    doc_ids.append(nid)\n",
        "    tok = tokenize(text)\n",
        "    corpus.append(tok)\n",
        "    text_map[nid] = text\n",
        "\n",
        "bm25 = BM25Okapi(corpus)\n",
        "\n",
        "with open(\"bm25_index.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\"bm25\": bm25, \"doc_ids\": doc_ids, \"text_map\": text_map}, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0680eb97",
      "metadata": {
        "id": "0680eb97"
      },
      "outputs": [],
      "source": [
        "embed_model = HuggingFaceEmbedding(model_name = \"QWEN3_4B_embedding\",\n",
        "                          device=\"cuda\" ,\n",
        "                          embed_batch_size=32,\n",
        "                          model_kwargs={\"torch_dtype\": \"float16\"},)\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(\"chromadb\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"RAG_EQ\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store,\n",
        "                       storage_context=storage_context,\n",
        "                       embed_model=embed_model,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7cf8a05",
      "metadata": {
        "id": "d7cf8a05"
      },
      "outputs": [],
      "source": [
        "txt_path = \"llm_log.txt\"\n",
        "\n",
        "def log_txt(user_id, user_query, node_texts, node_scores, llm_response,llm_memory_prompt=None):\n",
        "    times = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    node_strs = []\n",
        "    for i, (t, s) in enumerate(zip(node_texts, node_scores)):\n",
        "        node_strs.append(f\"Node {i+1} [Score: {s}]:\\n{t}\")\n",
        "    nodes_block = \"\\n\\n\".join(node_strs)\n",
        "    log_str = f\"\"\"==== User ID: {user_id} ====\n",
        "Timestamp: {times}\n",
        "User Query: {user_query}\n",
        "=========================\n",
        "Node Texts:\n",
        "{nodes_block}\n",
        "=========================\n",
        "LLM Full Prompt:\n",
        "{llm_memory_prompt}\n",
        "=========================\n",
        "LLM Response: {llm_response}\n",
        "----------------------------\n",
        "\n",
        "\"\"\"\n",
        "    with open(txt_path, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e6366c",
      "metadata": {
        "id": "b9e6366c"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from llama_index.core.schema import TextNode, NodeWithScore\n",
        "except Exception:\n",
        "    from llama_index.schema import TextNode, NodeWithScore  # 舊版相容\n",
        "\n",
        "# 查詢\n",
        "memory_prompt = (\n",
        "    \"\"\"你是一位機械維修工程師，請將以下使用者的對話整理成重點摘要，內容包含\\n\n",
        "    1.明確記錄使用者提到的設備型號與異常代碼\\n\n",
        "    2.明確記錄已經討論與檢查過的問題\\n\n",
        "    3.已經提供的建議與步驟\\n\n",
        "    4.後續追問需求\\n\n",
        "    5.請用條列式摘要\\n\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "memory = ChatSummaryMemoryBuffer(\n",
        "    llm=llm,\n",
        "    token_limit=2048,\n",
        "    max_token_limit=4096,\n",
        "    full_message_token_limit=1024,\n",
        "    summarize_prompt=memory_prompt\n",
        ")\n",
        "\n",
        "prompt = \"\"\"你是一位機械維修工程師，嚴格禁止輸出：推理步驟、內部分析、重複內容、英文\\n\n",
        "        必須輸出兩段，且都要有內容：\n",
        "        1.【檢索到的正確內容】：逐條摘錄證據的關鍵事實。\n",
        "        2.【可能的原因與處理方向】：根據證據條列原因；每條給出檢查→處置步驟。\n",
        "        \"\"\"\n",
        "\n",
        "with open(\"bm25_index.pkl\", \"rb\") as f:\n",
        "    bm25_pack = pickle.load(f)\n",
        "bm25 = bm25_pack[\"bm25\"]\n",
        "doc_ids = bm25_pack[\"doc_ids\"]\n",
        "text_map[nid] = text\n",
        "K = 60\n",
        "node_cache = {}  # nid -> Node,\n",
        "user_id = 1\n",
        "\n",
        "def get_node(obj):\n",
        "    return getattr(obj, \"node\", obj)\n",
        "\n",
        "def get_nid(obj):\n",
        "    n = get_node(obj)\n",
        "    return getattr(n, \"node_id\", getattr(n, \"id_\", None))\n",
        "\n",
        "def ensure_node(nid):\n",
        "    n = node_cache.get(nid)\n",
        "    if n is not None:\n",
        "        return n\n",
        "    try:\n",
        "        n = index.docstore.get_node(nid)\n",
        "    except Exception:\n",
        "        n = None\n",
        "    if n is None and nid in text_map:\n",
        "        n = TextNode(text=text_map[nid], id_=nid)\n",
        "    if n is not None:\n",
        "        node_cache[nid] = n\n",
        "    return n\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"請輸入問題（輸入 exit 結束）：\")\n",
        "    if not user_input.strip():\n",
        "        print(\"請輸入有效問題\")\n",
        "        continue\n",
        "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    question = user_input\n",
        "    meta_filters = parse_filters(question)\n",
        "\n",
        "    # ===== metadata filter =====\n",
        "    query_engine_filter = index.as_query_engine(\n",
        "        llm=llm,\n",
        "        memory=memory,\n",
        "        similarity_top_k=50,\n",
        "        filters=meta_filters if meta_filters else None\n",
        "    )\n",
        "    nodes_filter = query_engine_filter.retriever.retrieve(question) if meta_filters else []\n",
        "\n",
        "    # ===== Embedding =====\n",
        "    query_engine_emb = index.as_query_engine(\n",
        "        llm=llm,\n",
        "        memory=memory,\n",
        "        similarity_top_k=50,\n",
        "        filters=None\n",
        "    )\n",
        "    nodes_emb = query_engine_emb.retriever.retrieve(question)\n",
        "\n",
        "    # ===== 統一分數映射 =====\n",
        "    scores_by_id = defaultdict(float)\n",
        "    id_to_node = {}\n",
        "\n",
        "    # filter + emb 的 Node 與分數納入\n",
        "    for nws in (nodes_filter + nodes_emb):\n",
        "        nid = get_nid(nws)\n",
        "        if nid is None:\n",
        "            continue\n",
        "        n = get_node(nws)\n",
        "        id_to_node[nid] = n\n",
        "        s0 = float(getattr(nws, \"score\", 0.0) or 0.0)\n",
        "        if s0 > scores_by_id[nid]:\n",
        "            scores_by_id[nid] = s0\n",
        "\n",
        "    # ===== BM25 =====\n",
        "    qtoks = tokenize(question)  #\n",
        "    bm25_scores = bm25.get_scores(qtoks)\n",
        "    bm25_topk_idx = np.argsort(bm25_scores)[-50:][::-1]\n",
        "    bm25_hits = [(doc_ids[i], float(bm25_scores[i]), rank+1)\n",
        "                 for rank, i in enumerate(bm25_topk_idx)]\n",
        "\n",
        "    nodes_bm25 = []\n",
        "    for nid, s, r in bm25_hits:\n",
        "        n = ensure_node(nid)\n",
        "        if n is None:\n",
        "            continue\n",
        "        if s > scores_by_id.get(nid, 0.0):\n",
        "            scores_by_id[nid] = s\n",
        "        nodes_bm25.append(n)\n",
        "        id_to_node[nid] = n  #\n",
        "\n",
        "    node_source_type = {}\n",
        "    for nws in nodes_filter:\n",
        "        nid = get_nid(nws)\n",
        "        if nid:\n",
        "            node_source_type[nid] = \"filter\"\n",
        "    for nws in nodes_emb:\n",
        "        nid = get_nid(nws)\n",
        "        if nid:\n",
        "            node_source_type[nid] = \"hybrid\" if node_source_type.get(nid) == \"filter\" else \"embedding\"\n",
        "    for n in nodes_bm25:\n",
        "        nid = get_nid(n)\n",
        "        if nid:\n",
        "            prev = node_source_type.get(nid)\n",
        "            if prev:\n",
        "                if \"bm25\" not in prev:\n",
        "                    node_source_type[nid] = prev + \"_bm25\"\n",
        "            else:\n",
        "                node_source_type[nid] = \"bm25\"\n",
        "\n",
        "    # ===== 合併 =====\n",
        "    base_nodes_by_id = {}\n",
        "    for nws in (nodes_filter + nodes_emb):\n",
        "        n = get_node(nws)\n",
        "        nid = get_nid(nws)\n",
        "        if nid:\n",
        "            base_nodes_by_id[nid] = n\n",
        "    for n in nodes_bm25:\n",
        "        nid = get_nid(n)\n",
        "        if nid and nid not in base_nodes_by_id:\n",
        "            base_nodes_by_id[nid] = n\n",
        "\n",
        "    # ===== 最終 =====\n",
        "    def final_score(obj):\n",
        "        nid = get_nid(obj)\n",
        "        stype = node_source_type.get(nid, \"embedding\")\n",
        "        base = {\n",
        "            \"hybrid_bm25\" : 1.6,\n",
        "            \"filter_bm25\" : 1.3,\n",
        "            \"embedding_bm25\" : 1.2,\n",
        "            \"hybrid\" : 1.0,\n",
        "            \"filter\" : 0.6,\n",
        "            \"bm25\" : 0.5,\n",
        "            \"embedding\" : 0.0,\n",
        "        }.get(stype , 0.0)\n",
        "        raw = scores_by_id.get(nid , 0.0)\n",
        "        return raw + base\n",
        "\n",
        "    all_base_nodes = list(base_nodes_by_id.values())\n",
        "    if not all_base_nodes:\n",
        "        print(\"查無任何相關資料\\n查無相關資料，請補充查詢條件，如:機台編碼、異常原因或更細節的錯誤描述\")\n",
        "        continue\n",
        "\n",
        "    sorted_base_nodes = sorted(all_base_nodes , key=final_score, reverse=True)\n",
        "    top_base_nodes = sorted_base_nodes[:5]\n",
        "    top_nodes  = [NodeWithScore(node=n , score=final_score(n)) for n in top_base_nodes]\n",
        "\n",
        "    # ======================================================================================\n",
        "    top0_score = final_score(top_nodes[0])\n",
        "    if top0_score < 0.50:\n",
        "        response = \"查無相關資料，請補充查詢條件，如:機台編碼、異常原因或更細節的錯誤描述\"\n",
        "        print(\"查無任何相關資料\\n\", response)\n",
        "    elif top0_score < 0.70:\n",
        "        synthesizer = get_response_synthesizer(response_mode=\"compact\", llm=llm)\n",
        "        response = synthesizer.synthesize(query = question + \"\\n\" + prompt, nodes = top_nodes[:2])\n",
        "        print(\"目前查詢條件命中資料的相似度偏低，請補充查詢條件，如:機台編碼、異常原因或更細節的錯誤描述，以下為最相關兩筆資料供參考 :\\n\", response)\n",
        "    else:\n",
        "        synthesizer = get_response_synthesizer(response_mode=\"compact\", llm=llm)\n",
        "        response = synthesizer.synthesize(query=question + \"\\n\" + prompt, nodes=top_nodes)\n",
        "        print(response)\n",
        "\n",
        "    # ===== 記憶與紀錄 =====\n",
        "    message = [\n",
        "        ChatMessage(role=MessageRole.USER, content=user_input),\n",
        "        ChatMessage(role=MessageRole.ASSISTANT, content=response)\n",
        "    ]\n",
        "    memory.put_messages(message)\n",
        "\n",
        "    print(memory.get_all())\n",
        "    print(memory.chat_store)\n",
        "    print(memory)\n",
        "\n",
        "    memory_summary_text = memory.get()\n",
        "    node_texts  = [getattr(get_node(n), \"text\", \"\") for n in (top_nodes if top_base_nodes else [])]\n",
        "    node_scores = [final_score(n) for n in (top_nodes if top_base_nodes else [])]\n",
        "    log_txt(\n",
        "        user_id=user_id,\n",
        "        user_query=question,\n",
        "        node_texts=node_texts,\n",
        "        node_scores=node_scores,\n",
        "        llm_memory_prompt=memory_summary_text,\n",
        "        llm_response=response\n",
        "    )\n",
        "\n",
        "    # ===== 監控 BM25 缺失 =====\n",
        "    missing = [nid for nid, _, _ in bm25_hits if nid not in id_to_node]\n",
        "    if missing:\n",
        "        print(f\"[warn] 有 {len(missing)} 個 BM25 nid 不在 id_to_node（列前 5 個）: {missing[:5]}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}